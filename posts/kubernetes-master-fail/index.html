<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Kubernetes workers are not immune to master failures - blog</title><link rel="preload" href="http://bcoughlan.github.iofont/Inter-Medium.woff2" as="font" type="font/woff2" crossorigin>
	<link rel="preload" href="http://bcoughlan.github.iofont/Inter-Regular.woff2" as="font" type="font/woff2" crossorigin>
	<link rel="preload" href="http://bcoughlan.github.iofont/Playfair.woff2" as="font" type="font/woff2" crossorigin>

	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta property="og:title" content="Kubernetes workers are not immune to master failures" />
<meta property="og:description" content="There is some consensus floating around that Kubernetes worker applications can continue to function when the master is down:
Stack Overflow: What happens when the Kubernetes master fails (August 2016)
 It&rsquo;s my understanding that the master runs the API, and &hellip; when it is offline &hellip; life for applications will continue as normal unless nodes are rebooted, or there is a dramatic failure of some sort during this time, because TCP/ UDP services, load balancers, DNS, the dashboard, etc." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://bcoughlan.github.io/posts/kubernetes-master-fail/" />
<meta property="article:published_time" content="2020-02-22T01:51:37+00:00" />
<meta property="article:modified_time" content="2020-02-22T01:51:37+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Kubernetes workers are not immune to master failures"/>
<meta name="twitter:description" content="There is some consensus floating around that Kubernetes worker applications can continue to function when the master is down:
Stack Overflow: What happens when the Kubernetes master fails (August 2016)
 It&rsquo;s my understanding that the master runs the API, and &hellip; when it is offline &hellip; life for applications will continue as normal unless nodes are rebooted, or there is a dramatic failure of some sort during this time, because TCP/ UDP services, load balancers, DNS, the dashboard, etc."/>
<link rel="stylesheet" type="text/css" media="screen" href="http://bcoughlan.github.iocss/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="http://bcoughlan.github.iocss/main.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="http://bcoughlan.github.iocss/syntax.css" />
</head>

<body>
	<div class="container wrapper">
		<div class="header">
	
	<h1 class="site-title"><a href="http://bcoughlan.github.io">blog</a></h1>
	<div class="site-description"><nav class="nav social">
			<ul class="flat"></ul>
		</nav>
	</div>

	<nav class="nav">
		<ul class="flat">
			
		</ul>
	</nav>
</div>


		<div class="post">
			<div class="post-header">
				
					<div class="meta">
						<div class="date">
							<span class="day">22</span>
							<span class="rest">Feb 2020</span>
						</div>
					</div>
				
				<div class="matter">
					<h1 class="title">Kubernetes workers are not immune to master failures</h1>
				</div>
			</div>
					
			<div class="markdown">
				<p>There is some consensus floating around that Kubernetes worker applications can continue to function when the master is down:</p>
<p><a href="https://stackoverflow.com/questions/39172131/what-happens-when-the-kubernetes-master-fails/39173007#39173007">Stack Overflow: What happens when the Kubernetes master fails</a> (August 2016)</p>
<blockquote>
<p>It&rsquo;s my understanding that the master runs the API, and &hellip; when it is offline &hellip; life for applications will continue as normal unless nodes are rebooted, or there is a dramatic failure of some sort during this time, because TCP/ UDP services, load balancers, DNS, the dashboard, etc. Should all continue to function.</p>
</blockquote>
<p><a href="https://stackoverflow.com/questions/52619517/is-kubeadm-production-ready-now">Stack Overflow: Is kubeadm production ready now?</a> (October 2018)</p>
<blockquote>
<p>Also, keep in mind if your master(s) go down your workloads will keep running, you just won&rsquo;t be able to make changes or schedule new pods until the master(s) comes back up.</p>
</blockquote>
<p>This may lead you to conclude that the master node is not critical to application availability. For time, cost or complexity reasons you may think it&rsquo;s ok to run a single master, but let&rsquo;s see what happens when it goes down&hellip;</p>
<h2 id="breaking-dns">Breaking DNS</h2>
<p>Kubernetes runs its own DNS server to provide service discovery. If you set up your cluster with <code>kubeadm</code>, there is a deployment of CoreDNS with 2 replicas set up.  On a single-master setup this means that the master will run one CoreDNS pod and a worker will run the other (<a href="https://github.com/kubernetes/kubeadm/issues/1657">except when it doesn&rsquo;t</a>):</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ kubectl --namespace<span class="o">=</span>kube-system get pods -l k8s-app<span class="o">=</span>kube-dns -owide
NAME                      READY   STATUS    IP          NODE    
coredns-fb8b8dccf-pnvnn   1/1     Running   10.1.0.1   master 
coredns-fb8b8dccf-xs9d9   1/1     Running   10.1.0.2   worker1 </code></pre></div>
<p>If <code>worker1</code> goes down, the scheduler on the master will kick in and schedule a new DNS pod on another worker node. All good. But if your master goes down, the scheduler goes with it, and you&rsquo;ll be left with one DNS pod: only one node failure away from total failure.</p>
<p>But it&rsquo;s far worse than that. If the master goes down, 50% of the DNS requests will fail. To see why, we have to look at how DNS works in the pods:</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ kubectl run -i --tty busybox --image<span class="o">=</span>busybox --restart<span class="o">=</span>Never -- cat /etc/resolv.conf
nameserver 10.2.3.4
...</code></pre></div>
<p>Our pod&rsquo;s DNS points at an IP that matches neither of the CoreDNS pod IPs. In fact, it points to the virtual IP of a <code>Service</code>, named <code>kube-dns</code> for legacy reasons:</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">$ kubectl --namespace<span class="o">=</span>kube-system get service kube-dns
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>               
kube-dns   ClusterIP   10.2.3.4   &lt;none&gt;        53/UDP,53/TCP,9153/TCP

$ kubectl --namespace<span class="o">=</span>kube-system get endpoints kube-dns -o custom-columns<span class="o">=</span>IP:subsets<span class="o">[</span>*<span class="o">]</span>.addresses<span class="o">[</span>*<span class="o">]</span>.ip
IP
10.1.0.1,10.1.0.2</code></pre></div>
<p>A <code>Service</code> of type <code>ClusterIP</code> provides load balancing for pod-to-pod communication. The key to its operation is <code>kube-proxy</code>. The kube-proxy DaemonSet lives on every node, It listens to changes to Services and Endpoints from the kube-apiserver and updates iptables rules so that connections to the service IP are transparently routed to pods using NAT.</p>
<p>kube-proxy considers kube-apiserver to be the source of truth, and will not act on its own. So when the master goes down, kube-proxy will keep routing traffic to the CoreDNS pod on the master, causing half of the requests to fail and probably your applications along with it.</p>
<p>You could work around this problem by forcing CoreDNS to never run on the master node using taints and tolerations, but that would still only protect you from 2 out of N (nodes) failures. No single point of failure, but hardly resilient for any N &gt; 3.</p>
<h2 id="its-not-just-dns">It&rsquo;s not just DNS</h2>
<p>If your applications use Services for communication, then without the master you are still one node away from failure. A data center outage can take your multi-AZ application down. If you&rsquo;re deploying a very small cluster you may be tempted to remove the taints from a node so that it can schedule worker pods. Similar to the DNS problem, you are again vulnerable to the master failing.</p>
<h2 id="you-need-a-multi-master-setup-except-when">You need a multi-master setup, except when&hellip;</h2>
<p>The only time Kubernetes can be set up so that it&rsquo;s safe to run a single master is when every pod required for your application runs on the same set of nodes. This is a very specific requirement that is probably only worth the effort to very small users who want to save costs, self-hosted HA applications that bundle Kubernetes or those in the initial stages of migrating from a docker-compose setup. In any case, here&rsquo;s how to do it:</p>
<ol>
<li>Install Kubernetes 1.17+ with the <code>ServiceTopology</code> feature enabled. <a href="https://gist.github.com/bcoughlan/6f8649e36719664e3fc013a15789aa05">Here</a> is an example of how to do this with kubeadm.</li>
<li>Patch all of your <code>Service</code> objects so that they prefer routing to a local pod when one is available. See <a href="https://kubernetes.io/docs/concepts/services-networking/service-topology/#prefer-node-local-endpoints">this example</a> from the Kubernetes docs.</li>
<li>If using ingress-nginx, configure it to use the Service IP instead of proxying directly to the pods. See <a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#service-upstream">this example</a> from the ingress-nginx docs.</li>
<li>If you&rsquo;re using kubeadm instead of kops, patch CoreDNS so that it:
<ul>
<li>Doesn&rsquo;t crash when the master node crashes. See <a href="https://github.com/coredns/deployment/pull/138/files">patch</a> here.</li>
<li>Runs on every node. <a href="https://github.com/kubernetes/kops/blob/88ea28ac8cac8c15ca570bc7bf196005bb63395c/upup/models/cloudup/resources/addons/kube-dns.addons.k8s.io/k8s-1.12.yaml.template#L34">kops does this using cluster-proportional-autoscaler</a>.</li>
<li>No two pods run on the same node. kops does this <a href="https://github.com/kubernetes/kops/blob/88ea28ac8cac8c15ca570bc7bf196005bb63395c/upup/models/cloudup/resources/addons/kube-dns.addons.k8s.io/k8s-1.12.yaml.template#L111">using pod anti-affinity</a></li>
</ul>
</li>
</ol>
<h2 id="the-folk-wisdom-was-once-true">The folk wisdom was once true&hellip;</h2>
<p>The belief that workers could tolerate master failures was true until March 2016, when Kubernetes 1.2 was released. iptables became the default kube-proxy mode, replacing the user-space proxy mode. One key behavioral difference is this:</p>
<blockquote>
<p>If kube-proxy is running in iptables mode and the first Pod that’s selected does not respond, the connection fails. This is different from userspace mode: in that scenario, kube-proxy would detect that the connection to the first Pod had failed and would automatically retry with a different backend Pod.</p>
</blockquote>
<p>(from <a href="https://kubernetes.io/docs/concepts/services-networking/service/">https://kubernetes.io/docs/concepts/services-networking/service/</a>)</p>

			</div>

			<div class="tags">
				
					
				
			</div></div>
	</div>
	
</body>
</html>
